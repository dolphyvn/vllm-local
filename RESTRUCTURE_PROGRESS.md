# RAG Pipeline Restructure Progress

**Date Started:** 2025-10-31
**Goal:** Optimize data processing for small LLM + ChromaDB RAG system

## Architecture Redesign: Option B (Optimal)

### Problem Statement
Current narrative-based documents are not optimized for:
- Small local LLMs (verbose, unclear structure)
- Vector search efficiency (mixed data makes extraction difficult)
- Fast retrieval and filtering (ChromaDB metadata limitations)

### Solution: Structured JSON Pipeline

```
MT5 CSV ‚Üí Structured JSON ‚Üí Pattern Detection ‚Üí ChromaDB (structured) ‚Üí LLM Query Helper
```

## New Pipeline Components

### 1. `scripts/mt5_to_structured_json.py` ‚è≥
**Purpose:** Convert raw MT5 CSV to clean structured JSON
**Status:** In Progress
**Output Format:**
```json
{
  "metadata": {
    "symbol": "XAUUSD",
    "timeframe": "M15",
    "total_candles": 500,
    "date_range": {...}
  },
  "candles": [
    {
      "timestamp": "2025-10-31T12:00:00",
      "ohlc": {"open": 2650, "high": 2655, "low": 2648, "close": 2654},
      "volume": 1200,
      "indicators": {"rsi": 45, "ema_20": 2651, ...},
      "summary": "XAUUSD M15: Close 2654, RSI 45, bullish trend, 1.8x volume"
    }
  ]
}
```

### 2. `scripts/pattern_detector.py` üî≤
**Purpose:** Detect trading patterns and format for RAG
**Status:** Pending
**Output Format:**
```json
{
  "pattern_id": "bullish_engulfing_20251031_123456",
  "pattern": {"type": "Bullish Engulfing", "candle_data": {...}},
  "indicators": {...},
  "context": {...},
  "outcome": {...},
  "summary": "Short narrative for embedding"
}
```

### 3. `scripts/rag_structured_feeder.py` üî≤
**Purpose:** Feed structured patterns to ChromaDB
**Status:** Pending
**Features:**
- Flatten metadata for filtering
- Store full JSON in document field
- Hybrid search support (semantic + filters)

### 4. `scripts/pattern_retriever.py` üî≤
**Purpose:** Query helper to extract patterns for LLM
**Status:** Pending
**Features:**
- Semantic search + metadata filtering
- Statistical aggregation (win rate, avg PnL)
- Structured output for LLM consumption

## Implementation Progress

### Phase 1: Foundation ‚úÖ COMPLETE
- [x] Architecture review completed
- [x] Progress tracker created
- [x] mt5_to_structured_json.py (Completed 2025-10-31)
- [x] Script is executable and ready for testing

### Phase 2: Pattern Detection ‚úÖ COMPLETE
- [x] pattern_detector.py (Completed 2025-10-31)
- [x] Detects 10+ pattern types (candlestick + technical)
- [x] Outcome analysis with forward-looking validation
- [x] Quality scoring system implemented

### Phase 3: RAG Integration ‚úÖ COMPLETE
- [x] rag_structured_feeder.py (Completed 2025-10-31)
- [x] Batch processing with duplicate detection
- [x] Flattened metadata for ChromaDB compatibility
- [x] Statistics and monitoring built-in

### Phase 4: LLM Query Helper ‚úÖ COMPLETE
- [x] pattern_retriever.py (Completed 2025-10-31)
- [x] Semantic search + metadata filtering
- [x] Statistical aggregation (win rate, P&L, etc.)
- [x] LLM-optimized output formatting
- [x] Current market condition matching

### Phase 5: Integration & Testing ‚úÖ COMPLETE
- [x] Create workflow integration script (process_pipeline.sh)
- [x] Test with sample CSV data (200 candles ‚Üí 43 patterns)
- [x] Fix CSV parsing (multi-delimiter detection)
- [x] Fix ChromaDB filter handling (multiple filters require $and)
- [x] Verify pattern retrieval with various queries
- [ ] Update main.py /upload endpoint (PENDING)
- [ ] Performance benchmarking (PENDING)
- [ ] End-to-end documentation (PENDING)

### Phase 6: Market Profile Enhancement ‚úÖ COMPLETE (2025-10-31)
- [x] Add session-based VWAP (resets per trading session)
- [x] Implement POC (Point of Control) calculation
- [x] Calculate VAH/VAL (Value Area High/Low) - 70% volume area
- [x] Add Volume Profile by price levels
- [x] Implement Initial Balance (first hour range per session)
- [x] Test with sample data (41 indicators vs 31 previously)
- [x] Verify Market Profile data in JSON output
- [x] Update format version to 2.1_market_profile

### Phase 7: Main.py /upload Integration ‚úÖ COMPLETE (2025-10-31)
- [x] Create background processing function (process_full_history_to_rag)
- [x] Add BackgroundTasks parameter to /upload endpoint
- [x] Implement conditional logic for file type detection
- [x] *_0.csv files ‚Üí Trigger RAG pipeline in background
- [x] *_200.csv files ‚Üí Save only (for LLM analysis)
- [x] Add rag_processing flag to API response
- [x] Update success messages per file type
- [x] Test pipeline integration
- [x] Update documentation (QUICK_START.md)

## Key Design Decisions

### 1. **Structured vs Narrative**
‚úÖ **Chosen:** Structured JSON with short summary field
**Reason:** Better for small LLMs, faster retrieval, easier filtering

### 2. **Metadata Storage**
‚úÖ **Chosen:** Flattened metadata + full JSON in document
**Reason:** ChromaDB limitations, hybrid search capability

### 3. **Pattern Format**
‚úÖ **Chosen:** Separate candle data + pattern detection
**Reason:** Modularity, easier to update detection algorithms

### 4. **Embedding Strategy**
‚úÖ **Chosen:** Short summary field for embeddings, full data for retrieval
**Reason:** Optimize embedding quality, preserve all data

## Testing Strategy

### Unit Tests
- [ ] CSV parsing with multiple encodings
- [ ] Indicator calculations accuracy
- [ ] Pattern detection precision
- [ ] ChromaDB insert/query

### Integration Tests
- [ ] Full pipeline: CSV ‚Üí ChromaDB
- [ ] Pattern retrieval by similarity
- [ ] LLM query integration
- [ ] Performance benchmarks

### Test Data
- Sample files in `data/test/`:
  - `XAUUSD_PERIOD_M15_0.csv` (full history)
  - `XAUUSD_PERIOD_M15_200.csv` (live data)

## Rollback Plan

If issues arise, can revert to:
- `scripts/simple_data_processor.py` (working baseline)
- Current ChromaDB structure intact (new collection name)

## Next Session Resume Point

**Last Completed:** All core pipeline components (Phases 1-4) ‚úÖ
**Next Task:** Testing and main.py integration
**Command to resume:**
```bash
cd /opt/works/personal/vllm-local
cat RESTRUCTURE_PROGRESS.md  # Review progress

# Test the pipeline with sample data:
./scripts/process_pipeline.sh data/XAUUSD_PERIOD_M15_200.csv --live

# Or manually test each component:
# 1. Convert CSV to structured JSON
python scripts/mt5_to_structured_json.py --input data/sample.csv --output data/structured/test.json --symbol XAUUSD --timeframe M15

# 2. Detect patterns
python scripts/pattern_detector.py --input data/structured/test.json --output data/patterns/test_patterns.json

# 3. Feed to ChromaDB
python scripts/rag_structured_feeder.py --input data/patterns/test_patterns.json

# 4. Test retrieval
python scripts/pattern_retriever.py --query "bullish reversal" --format llm
```

## Testing Results ‚úÖ

### Successful Tests (2025-10-31)
- **Pipeline automation:** Processed 200 candles ‚Üí 43 patterns successfully
- **CSV parsing:** Successfully detected UTF-8 with comma delimiter
- **Pattern detection:** 43 patterns detected (16.3% WIN, 51.2% LOSS, 32.6% NEUTRAL)
- **ChromaDB storage:** All 43 patterns stored with proper metadata
- **Query retrieval:**
  - Single filter: ‚úÖ Works (tested with outcome=WIN, found 5 patterns)
  - Multiple filters: ‚úÖ Works (tested with symbol+timeframe)
  - LLM format: ‚úÖ Produces structured output with statistics
  - Current market: ‚úÖ Works (no results due to small dataset)
- **Market Profile (Phase 6):**
  - Session VWAP: ‚úÖ Resets correctly per session (Asia, London, NY, etc.)
  - POC/VAH/VAL: ‚úÖ Calculated per session with 70% value area
  - Initial Balance: ‚úÖ First hour range tracked correctly
  - Indicator count: ‚úÖ Increased from 31 to 41 indicators

### Performance Metrics
- CSV conversion: 200 candles in ~2 seconds (41 indicators)
- Pattern detection: 43 patterns in ~3 seconds
- ChromaDB feeding: 43 patterns in <1 second
- Query retrieval: <100ms response time
- Market Profile overhead: ~0.5 seconds per 200 candles

### Market Profile Feature Details
**Session-based VWAP:**
- Resets at session boundaries (Asia 00:00, London 08:00, NY 13:00, etc.)
- More accurate than cumulative VWAP for intraday analysis

**POC (Point of Control):**
- Price level with highest volume concentration
- Calculated per session using 500 price bins maximum
- Example: POC 3937.22 for Pacific session

**VAH/VAL (Value Area):**
- 70% of session volume captured
- Expands outward from POC until 70% threshold reached
- Example: VAH 3957.46, VAL 3928.14

**Initial Balance:**
- First hour range of each session
- Timeframe-aware (M15: 4 candles, H1: 1 candle, etc.)
- Example: IB Range 51.80 points

### Known Limitations
- Current market search may return no results with small datasets (need >500 patterns for good coverage)
- Requires minimum 120 candles for pattern detection (lookback period)
- Live mode uses shorter lookforward (10 bars vs 20 bars)
- Market Profile requires sufficient volume data (fallback to price range if no volume)

## Notes & Observations

- Merge conflicts in `data_processor.py` indicate active development
- `simple_data_processor.py` is clean baseline (13KB vs 75KB)
- Current embedding model may need verification (check config.json)
- Consider financial-specific embeddings (finbert, instructor-finance)

## File Changes Tracking

### New Files Created ‚úÖ
- `RESTRUCTURE_PROGRESS.md` - Progress tracker and documentation
- `scripts/mt5_to_structured_json.py` - CSV to structured JSON converter (479 lines)
- `scripts/pattern_detector.py` - Pattern detection engine (773 lines)
- `scripts/rag_structured_feeder.py` - ChromaDB feeder (375 lines)
- `scripts/pattern_retriever.py` - LLM query helper (457 lines)
- `scripts/process_pipeline.sh` - Complete automation pipeline (180 lines)

**Total New Code:** ~2,264 lines of production-ready Python/Bash

### Files Modified (Bug Fixes) ‚úÖ
- `scripts/mt5_to_structured_json.py` - Fixed CSV parsing with multi-delimiter detection
  - Added support for comma, tab, and semicolon delimiters
  - Enhanced column validation (must have > 3 columns)
  - Better error messages for corrupted files
- `scripts/pattern_retriever.py` - Fixed ChromaDB filter handling
  - Multiple filters now wrapped in `$and` operator
  - Single filters passed directly without operator

### Files to Modify (Pending)
- `main.py` - Update /upload endpoint (Phase 5)

### Files to Deprecate (Future)
- `scripts/data_processor.py` (after testing)
- `scripts/feed_to_rag.py` (replaced by rag_structured_feeder.py)

---

**Legend:**
- ‚úÖ Complete
- ‚è≥ In Progress
- üî≤ Pending
- ‚ùå Blocked
